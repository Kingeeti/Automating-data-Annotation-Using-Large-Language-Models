{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac4153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 38 year links.\n",
      "🔍 Scraping https://papers.nips.cc/paper_files/paper/2020: Found 0 papers.\n",
      "🔍 Scraping https://papers.nips.cc/paper_files/paper/2021: Found 0 papers.\n",
      "🔍 Scraping https://papers.nips.cc/paper_files/paper/2023: Found 3218 papers.\n",
      "🔍 Scraping https://papers.nips.cc/paper_files/paper/2022: Found 2671 papers.\n",
      "🔍 Scraping https://papers.nips.cc/paper_files/paper/2024: Found 4035 papers.\n",
      "✅ Successfully saved 9924 papers to database.\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()  \n",
    "\n",
    "BASE_URL = \"https://papers.nips.cc\"\n",
    "DB_FILE = r\"C:\\kinge\\neurips_papers.db\"  \n",
    "\n",
    "async def fetch(session, url):\n",
    "    \"\"\"Fetch page content asynchronously.\"\"\"\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def get_year_links(session):\n",
    "    \"\"\"Extract valid NeurIPS proceedings year links.\"\"\"\n",
    "    url = BASE_URL  \n",
    "    html = await fetch(session, url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    year_links = [\n",
    "        BASE_URL + a[\"href\"]\n",
    "        for a in soup.select(\"a[href^='/paper_files/paper/']\")\n",
    "    ]\n",
    "\n",
    "    print(f\"✅ Found {len(year_links)} year links.\")\n",
    "    return year_links[:5]  \n",
    "\n",
    "async def parse_papers(year_url, session):\n",
    "    \"\"\"Extract paper titles and links from a given year's page.\"\"\"\n",
    "    papers = []\n",
    "    html = await fetch(session, year_url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    paper_elements = soup.select(\"a[title][href*='Abstract-Conference.html']\")\n",
    "\n",
    "    print(f\"🔍 Scraping {year_url}: Found {len(paper_elements)} papers.\")  \n",
    "    for paper in paper_elements:\n",
    "        paper_title = paper[\"title\"].strip() \n",
    "        paper_link = BASE_URL + paper[\"href\"]  \n",
    "        papers.append((paper_title, paper_link))\n",
    "    \n",
    "    return papers\n",
    "\n",
    "async def scrape_neurips():\n",
    "    \"\"\"Main function to scrape NeurIPS papers.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        year_links = await get_year_links(session)\n",
    "        \n",
    "        if not year_links:\n",
    "            print(\"❌ No year links found. The website structure might have changed.\")\n",
    "            return\n",
    "\n",
    "        tasks = [parse_papers(year_url, session) for year_url in year_links]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        all_papers = [paper for result in results for paper in result] \n",
    "        save_to_db(all_papers)\n",
    "\n",
    "def save_to_db(papers):\n",
    "    \"\"\"Save extracted papers to SQLite database.\"\"\"\n",
    "    if not papers:\n",
    "        print(\"❌ No papers found. Database not updated.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(DB_FILE), exist_ok=True)\n",
    "        conn = sqlite3.connect(DB_FILE)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS papers (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                title TEXT,\n",
    "                link TEXT UNIQUE\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.executemany(\"INSERT OR IGNORE INTO papers (title, link) VALUES (?, ?)\", papers)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"✅ Successfully saved {len(papers)} papers to database.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Database Error: {e}\")\n",
    "\n",
    "await scrape_neurips()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211497d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'paper title', 'https://papers.nips.cc/paper_files/paper/2024/hash/000f947dcaff8fbffcc3f53a1314f358-Abstract-Conference.html')\n",
      "(2, 'paper title', 'https://papers.nips.cc/paper_files/paper/2024/hash/00295cede6e1600d344b5cd6d9fd4640-Abstract-Conference.html')\n",
      "(3, 'paper title', 'https://papers.nips.cc/paper_files/paper/2024/hash/00532321a253959cedc4f971b5524131-Abstract-Conference.html')\n",
      "(4, 'paper title', 'https://papers.nips.cc/paper_files/paper/2024/hash/005413e90d003d13886019607b037f52-Abstract-Conference.html')\n",
      "(5, 'paper title', 'https://papers.nips.cc/paper_files/paper/2024/hash/00616a2d48f5716f3d6f783491149364-Abstract-Conference.html')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "DB_FILE = r\"C:\\kinge\\neurips_papers.db\"\n",
    "\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM papers LIMIT 5\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932aefd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_api_key(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "OPENAI_API_KEY = load_api_key(\"api_key.txt\") \n",
    "\n",
    "print(\"✅ API Key Loaded Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas\n",
    "import sqlite3  \n",
    "\n",
    "print(\"✅ All libraries installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'category' column added successfully.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "DB_FILE = \"neurips_papers.db\"\n",
    "\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    cursor.execute(\"ALTER TABLE papers ADD COLUMN category TEXT\")\n",
    "    print(\"✅ 'category' column added successfully.\")\n",
    "except sqlite3.OperationalError:\n",
    "    print(\"⚠️ 'category' column already exists.\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ce78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Papers successfully reloaded into the database!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "CSV_FILE = \"neurips_papers.csv\"  \n",
    "DB_FILE = \"neurips_papers.db\"\n",
    "\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "if \"title\" not in df.columns:\n",
    "    print(\"🚨 The CSV does not contain a 'title' column!\")\n",
    "    exit()\n",
    "\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS papers (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT NOT NULL,\n",
    "        category TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(\"INSERT INTO papers (title) VALUES (?)\", (row[\"title\"],))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ Papers successfully reloaded into the database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af546b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Total Papers in Database: 9924\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"neurips_papers.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM papers\")\n",
    "paper_count = cursor.fetchone()[0]\n",
    "\n",
    "print(f\"📄 Total Papers in Database: {paper_count}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d5c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 10 papers to annotate...\n",
      "📌 Annotated: paper title → Computer Vision\n",
      "📌 Annotated: paper title → Natural Language Processing (NLP)\n",
      "📌 Annotated: paper title → Natural Language Processing (NLP)\n",
      "📌 Annotated: paper title → Computer Vision\n",
      "📌 Annotated: paper title → Computer Vision\n",
      "📌 Annotated: paper title → Computer Vision\n",
      "📌 Annotated: paper title → Computer Vision\n",
      "📌 Annotated: paper title → Computer Vision\n",
      "📌 Annotated: paper title → Optimization\n",
      "📌 Annotated: paper title → Computer Vision\n",
      "🎉 Annotation process completed!\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import sqlite3\n",
    "\n",
    "with open(\"api_key.txt\", \"r\") as f:\n",
    "    API_KEY = f.read().strip()\n",
    "\n",
    "DB_FILE = \"neurips_papers.db\"\n",
    "CATEGORIES = [\n",
    "    \"Deep Learning\", \"Natural Language Processing (NLP)\",\n",
    "    \"Reinforcement Learning\", \"Optimization\", \"Computer Vision\", \"Other\"\n",
    "]\n",
    "\n",
    "async def classify_paper(title):\n",
    "    \"\"\"Force Gemini to classify the paper into one of the categories.\"\"\"\n",
    "    url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    params = {\"key\": API_KEY}\n",
    "\n",
    "    prompt_text = (\n",
    "        \"**IMPORTANT INSTRUCTIONS:** You MUST classify the following research paper title \"\n",
    "        \"into exactly ONE of these categories: \\n\"\n",
    "        \"1. Deep Learning\\n\"\n",
    "        \"2. Natural Language Processing (NLP)\\n\"\n",
    "        \"3. Reinforcement Learning\\n\"\n",
    "        \"4. Optimization\\n\"\n",
    "        \"5. Computer Vision\\n\\n\"\n",
    "        \"**RULES:**\\n\"\n",
    "        \"- Return ONLY the category name, with NO extra text.\\n\"\n",
    "        \"- If unsure, choose the CLOSEST matching category.\\n\"\n",
    "        \"- NEVER return 'Other' unless it truly does not fit any category.\\n\\n\"\n",
    "        f\"**Title:** \\\"{title}\\\"\\n\"\n",
    "        \"**Category:**\"\n",
    "    )\n",
    "\n",
    "    data = {\"contents\": [{\"parts\": [{\"text\": prompt_text}]}]}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, headers=headers, params=params, json=data) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"🚨 Error classifying '{title}': {await response.text()}\")\n",
    "                return \"Other\"\n",
    "\n",
    "            result = await response.json()\n",
    "            try:\n",
    "                category = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].strip()\n",
    "\n",
    "                return category if category in CATEGORIES else \"Other\"\n",
    "            except (KeyError, IndexError):\n",
    "                return \"Other\"\n",
    "\n",
    "def fetch_papers():\n",
    "    \"\"\"Retrieve unannotated papers from the database.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT id, title FROM papers WHERE category IS NULL LIMIT 10\")\n",
    "    papers = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return papers\n",
    "\n",
    "def update_paper_category(paper_id, category):\n",
    "    \"\"\"Update the category of a paper in the database.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"UPDATE papers SET category = ? WHERE id = ?\", (category, paper_id))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "async def annotate_papers():\n",
    "    \"\"\"Fetch papers, classify them, and update the database.\"\"\"\n",
    "    papers = fetch_papers()\n",
    "    if not papers:\n",
    "        print(\"✅ No new papers to annotate.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🔍 Found {len(papers)} papers to annotate...\")\n",
    "\n",
    "    for paper_id, title in papers:\n",
    "        category = await classify_paper(title)\n",
    "        update_paper_category(paper_id, category)\n",
    "        print(f\"📌 Annotated: {title} → {category}\")\n",
    "\n",
    "    print(\"🎉 Annotation process completed!\")\n",
    "\n",
    "asyncio.run(annotate_papers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "931b2f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Dataset saved as 'neurips_papers_annotated.csv' ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "DB_FILE = \"neurips_papers.db\"\n",
    "OUTPUT_CSV = \"neurips_papers_annotated.csv\"\n",
    "\n",
    "def export_to_csv():\n",
    "    \"\"\"Export the annotated dataset to a CSV file.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM papers WHERE category IS NOT NULL\", conn)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    conn.close()\n",
    "    print(f\"📁 Dataset saved as '{OUTPUT_CSV}' ✅\")\n",
    "\n",
    "export_to_csv()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
